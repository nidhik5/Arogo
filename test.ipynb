{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Medical Report: Describe this medical image:nastrntstrntstrntstrntstrntstrntstrntstrnts\n",
      "User Query: What are the abnormalities?\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from transformers import CLIPProcessor, CLIPModel, T5ForConditionalGeneration, T5Tokenizer\n",
    "from PIL import Image\n",
    "import os\n",
    "import re\n",
    "\n",
    "# 1. Image Preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image_tensor = transform(image)\n",
    "    image_tensor = torch.clamp(image_tensor, 0, 1)\n",
    "    return image_tensor.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "# 2. Text Preprocessing\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "def tokenize_text(text, tokenizer):\n",
    "    return tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "\n",
    "# 3. Model Setup\n",
    "## Image Encoder (CLIP Model)\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "## Text Decoder (T5)\n",
    "t5_model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
    "\n",
    "# 4. Multimodal Fusion and Text Generation\n",
    "def generate_caption(image_path):\n",
    "    # Preprocess image\n",
    "    image = preprocess_image(image_path)\n",
    "    inputs = clip_processor(images=image, return_tensors=\"pt\", do_rescale=False)\n",
    "\n",
    "    # Extract features from CLIP model\n",
    "    with torch.no_grad():\n",
    "        image_features = clip_model.get_image_features(**inputs)\n",
    "\n",
    "    # Prepare text input\n",
    "    prompt = \"Describe this medical image:\"\n",
    "    input_ids = t5_tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "    # Get initial decoder input\n",
    "    decoder_input_ids = t5_model._shift_right(input_ids)\n",
    "\n",
    "    # Project image features to match text embedding size\n",
    "    image_features_proj = nn.Linear(image_features.shape[-1], t5_model.config.d_model)(image_features)\n",
    "    image_features_proj = image_features_proj.unsqueeze(1)  # Add sequence dimension\n",
    "\n",
    "    # Generate the caption\n",
    "    outputs = t5_model.generate(\n",
    "        input_ids=input_ids,\n",
    "        decoder_input_ids=decoder_input_ids,\n",
    "        encoder_outputs=torch.stack([image_features_proj]),\n",
    "        max_length=50\n",
    "    )\n",
    "\n",
    "    caption = t5_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return caption\n",
    "\n",
    "# 5. Fine-Tuning Process (Placeholder)\n",
    "def fine_tune_model():\n",
    "    pass  # Placeholder for future implementation\n",
    "\n",
    "# 6. Chatbot Integration\n",
    "def chatbot_response(user_input, image_path):\n",
    "    caption = generate_caption(image_path)\n",
    "    response = f\"Generated Medical Report: {caption}\\nUser Query: {user_input}\"\n",
    "    return response\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    image_path = r\"C:/Users/admin/Downloads/chest2.jpeg\"\n",
    "    user_query = \"What are the abnormalities?\"\n",
    "    print(chatbot_response(user_query, image_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in a:\\vishal\\myenv\\lib\\site-packages (0.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "f-string expression part cannot include a backslash (97095523.py, line 81)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[2], line 81\u001b[1;36m\u001b[0m\n\u001b[1;33m    print(f\"Loading image {idx}: {os.path.join(item['location'], figure['graphic_ref'].split('\\\\')[-1])}\")\u001b[0m\n\u001b[1;37m                                                                                                         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m f-string expression part cannot include a backslash\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, CLIPProcessor, CLIPModel, AdamW\n",
    "from PIL import Image\n",
    "import json\n",
    "import os\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "class ImageToTextProjection(nn.Module):\n",
    "    def __init__(self, input_dim=512, output_dim=768):\n",
    "        super().__init__()\n",
    "        # Simplified architecture - direct projection\n",
    "        self.projection = nn.Linear(input_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.projection(x).unsqueeze(1)\n",
    "\n",
    "class MedicalReportDataset(Dataset):\n",
    "    def __init__(self, jsonl_file, clip_processor, t5_tokenizer, max_samples=None):\n",
    "        self.clip_processor = clip_processor\n",
    "        self.t5_tokenizer = t5_tokenizer\n",
    "        \n",
    "        # Add counters for debugging\n",
    "        total_entries = 0\n",
    "        valid_entries = 0\n",
    "        error_types = {}\n",
    "        \n",
    "        self.data = []\n",
    "        with open(jsonl_file, \"r\") as f:\n",
    "            for line in f:\n",
    "                total_entries += 1\n",
    "                entry = json.loads(line)\n",
    "                \n",
    "                # Check if entry has figures\n",
    "                if not entry.get(\"figures\"):\n",
    "                    error_types[\"no_figures\"] = error_types.get(\"no_figures\", 0) + 1\n",
    "                    continue\n",
    "                    \n",
    "                figure = entry[\"figures\"][0]\n",
    "                image_path = os.path.join(entry[\"location\"], figure[\"graphic_ref\"].split(\"\\\\\")[-1])\n",
    "                \n",
    "                # Check if file exists\n",
    "                if not os.path.exists(image_path):\n",
    "                    error_types[\"missing_file\"] = error_types.get(\"missing_file\", 0) + 1\n",
    "                    continue\n",
    "                \n",
    "                # Check if we can open the image\n",
    "                try:\n",
    "                    with Image.open(image_path) as img:\n",
    "                        pass\n",
    "                    valid_entries += 1\n",
    "                    self.data.append(entry)\n",
    "                except Exception as e:\n",
    "                    error_type = str(type(e).__name__)\n",
    "                    error_types[error_type] = error_types.get(error_type, 0) + 1\n",
    "                    continue\n",
    "                \n",
    "                if max_samples and valid_entries >= max_samples:\n",
    "                    break\n",
    "        \n",
    "        print(f\"\\nDataset Loading Statistics:\")\n",
    "        print(f\"Total entries in JSONL: {total_entries}\")\n",
    "        print(f\"Valid entries found: {valid_entries}\")\n",
    "        print(\"\\nError breakdown:\")\n",
    "        for error_type, count in error_types.items():\n",
    "            print(f\"{error_type}: {count}\")\n",
    "        print(\"\\nExample valid image path:\", self.data[0][\"figures\"][0][\"graphic_ref\"] if self.data else \"No valid images found\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        figure = item[\"figures\"][0]\n",
    "        \n",
    "        # Print the first few paths for debugging\n",
    "        if idx < 5:\n",
    "            print(f\"Loading image {idx}: {os.path.join(item['location'], figure['graphic_ref'].split('\\\\')[-1])}\")\n",
    "            \n",
    "        image_path = os.path.join(item[\"location\"], figure[\"graphic_ref\"].split(\"\\\\\")[-1])\n",
    "        caption = figure[\"fig_caption\"]\n",
    "\n",
    "        try:\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "            pixel_values = self.clip_processor(images=image, return_tensors=None)[\"pixel_values\"][0]\n",
    "            labels = self.t5_tokenizer(caption, max_length=128, truncation=True)[\"input_ids\"]\n",
    "            \n",
    "            return {\n",
    "                \"pixel_values\": torch.tensor(pixel_values),\n",
    "                \"labels\": torch.tensor(labels)\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {image_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "def collate_fn(batch):\n",
    "    batch = [item for item in batch if item is not None]\n",
    "    if not batch:\n",
    "        return None\n",
    "    \n",
    "    pixel_values = torch.stack([item[\"pixel_values\"] for item in batch])\n",
    "    labels = torch.nn.utils.rnn.pad_sequence(\n",
    "        [item[\"labels\"] for item in batch], \n",
    "        batch_first=True, \n",
    "        padding_value=-100\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"pixel_values\": pixel_values,\n",
    "        \"labels\": labels\n",
    "    }\n",
    "\n",
    "def train_model(dataloader, clip_model, t5_model, projection_layer, optimizer, scaler, device, max_batches=None):\n",
    "    total_loss = 0\n",
    "    batch_count = 0\n",
    "    \n",
    "    # Use tqdm for progress bar\n",
    "    pbar = tqdm(dataloader, desc=\"Training\", total=min(len(dataloader), max_batches) if max_batches else len(dataloader))\n",
    "    \n",
    "    for batch in pbar:\n",
    "        if batch is None:\n",
    "            continue\n",
    "            \n",
    "        if max_batches and batch_count >= max_batches:\n",
    "            break\n",
    "            \n",
    "        # Move batch to device\n",
    "        pixel_values = batch[\"pixel_values\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        # Use automatic mixed precision\n",
    "        with autocast():\n",
    "            # Get CLIP features\n",
    "            with torch.no_grad():\n",
    "                image_features = clip_model.get_image_features(pixel_values=pixel_values)\n",
    "\n",
    "            # Project features and get T5 outputs\n",
    "            projected_features = projection_layer(image_features)\n",
    "            outputs = t5_model(inputs_embeds=projected_features, labels=labels)\n",
    "            loss = outputs.loss\n",
    "\n",
    "        # Scaled backward pass\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        batch_count += 1\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "        \n",
    "        # Clear cache periodically\n",
    "        if batch_count % 10 == 0:\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "    return total_loss / batch_count if batch_count > 0 else 0\n",
    "\n",
    "def main():\n",
    "    # Set device and enable benchmarking\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "    # Initialize models\n",
    "    clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "    t5_tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
    "    t5_model = T5ForConditionalGeneration.from_pretrained(\"t5-base\").to(device)\n",
    "    \n",
    "    # Simplified projection layer\n",
    "    projection_layer = ImageToTextProjection().to(device)\n",
    "    \n",
    "    # Create dataset with limited samples for testing\n",
    "    jsonl_file = \"A:\\Vishal\\BiomedCLIP_data_pipeline\\_results\\data\\pubmed_parsed_data.jsonl\"\n",
    "    dataset = MedicalReportDataset(\n",
    "        jsonl_file, \n",
    "        clip_processor, \n",
    "        t5_tokenizer,\n",
    "        max_samples=1000  # Limit samples for testing\n",
    "    )\n",
    "    \n",
    "    # Create dataloader with increased num_workers\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=16,  # Increased batch size\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=4,  # Adjust based on your CPU\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # Initialize optimizer and gradient scaler\n",
    "    optimizer = AdamW(\n",
    "        list(t5_model.parameters()) + list(projection_layer.parameters()),\n",
    "        lr=5e-5,\n",
    "        weight_decay=0.01\n",
    "    )\n",
    "    scaler = GradScaler()\n",
    "    \n",
    "    # Training loop\n",
    "    num_epochs = 3\n",
    "    max_batches_per_epoch = 100  # Limit batches per epoch for testing\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "        \n",
    "        # Set models to appropriate modes\n",
    "        clip_model.eval()\n",
    "        t5_model.train()\n",
    "        projection_layer.train()\n",
    "        \n",
    "        # Train one epoch\n",
    "        avg_loss = train_model(\n",
    "            dataloader, \n",
    "            clip_model, \n",
    "            t5_model, \n",
    "            projection_layer, \n",
    "            optimizer, \n",
    "            scaler, \n",
    "            device,\n",
    "            max_batches=max_batches_per_epoch\n",
    "        )\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        # Save checkpoint\n",
    "        if (epoch + 1) % 1 == 0:\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                't5_model_state_dict': t5_model.state_dict(),\n",
    "                'projection_layer_state_dict': projection_layer.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': avg_loss,\n",
    "            }, f'checkpoint_epoch_{epoch+1}.pth')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Model weights loaded successfully!\n",
      "Generated Medical Report: a detailed medical report for a chest X-ray. Generate a detailed medical report for a chest X-ray.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModel, T5Tokenizer, T5ForConditionalGeneration, CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.simplefilter(\"ignore\", FutureWarning)\n",
    "\n",
    "# Set device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "### Step 1: Load BiomedVLP-CXR-BERT-general Model for Text Embeddings ###\n",
    "bert_model_name = \"microsoft/BiomedVLP-CXR-BERT-general\"\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\n",
    "bert_model = AutoModel.from_pretrained(bert_model_name).to(device)\n",
    "\n",
    "### Step 2: Load CLIP Model for Image Feature Extraction ###\n",
    "class MedCLIPModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MedCLIPModel, self).__init__()\n",
    "        self.model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    \n",
    "    def forward(self, image):\n",
    "        return self.model.get_image_features(image)\n",
    "\n",
    "# Function to load MedCLIP model\n",
    "def load_medclip_model():\n",
    "    try:\n",
    "        model = MedCLIPModel().to(device)\n",
    "        print(\"Image Model weights loaded successfully!\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading MedCLIP model: {e}\")\n",
    "        return None\n",
    "\n",
    "# Image processing function\n",
    "def preprocess_image(image_path, processor):\n",
    "    try:\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        pixel_values = processor(images=image, return_tensors=\"pt\").pixel_values\n",
    "        if pixel_values is None:\n",
    "            raise ValueError(\"Image preprocessing failed. Check input format.\")\n",
    "        return pixel_values.to(device)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image: {e}\")\n",
    "        return None\n",
    "\n",
    "### Step 3: Load T5 Model for Medical Report Generation ###\n",
    "t5_model_name = \"t5-base\"  # Use a fine-tuned T5 medical model if available\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained(t5_model_name)\n",
    "t5_model = T5ForConditionalGeneration.from_pretrained(t5_model_name).to(device)\n",
    "\n",
    "### Step 4: Generate Medical Caption from Image and Text ###\n",
    "def generate_medical_report(image_path, medclip_model, processor, bert_model, bert_tokenizer, t5_model, t5_tokenizer):\n",
    "    if medclip_model is None:\n",
    "        raise RuntimeError(\"MedCLIP model is not initialized. Check model loading.\")\n",
    "\n",
    "    # Extract Image Features (not used in this version)\n",
    "    pixel_values = preprocess_image(image_path, processor)\n",
    "    if pixel_values is None:\n",
    "        return \"Error: Image processing failed.\"\n",
    "\n",
    "    # Simplified prompt for T5\n",
    "    prompt = \"Generate a detailed medical report for a chest X-ray.\"\n",
    "    t5_inputs = t5_tokenizer(prompt, return_tensors=\"pt\", max_length=512, truncation=True).to(device)\n",
    "    output_ids = t5_model.generate(**t5_inputs, max_length=150)\n",
    "    caption = t5_tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    return caption\n",
    "\n",
    "### Load Models ###\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "medclip_model = load_medclip_model()\n",
    "\n",
    "### Provide Image Path ###\n",
    "image_path = \"C:/Users/admin/Downloads/chest2.jpeg\"  # Replace with your image path\n",
    "caption = generate_medical_report(image_path, medclip_model, processor, bert_model, bert_tokenizer, t5_model, t5_tokenizer)\n",
    "print(\"Generated Medical Report:\", caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
