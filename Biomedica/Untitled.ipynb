{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cd903522-19f0-4b9c-accc-147b334c429d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from huggingface_hub import login, get_token, hf_hub_url\n",
    "# import webdataset as wds\n",
    "# import requests\n",
    "# from torch.utils.data import IterableDataset\n",
    "# import tarfile\n",
    "# import io\n",
    "# from PIL import Image\n",
    "# import IPython.display as display\n",
    "# import os\n",
    "\n",
    "\n",
    "# # Login to Hugging Face\n",
    "# login()\n",
    "# hf_token = get_token()\n",
    "# print(\"Logged in to Hugging Face!\")\n",
    "\n",
    "\n",
    "# # Dataset configuration\n",
    "# repo_id = \"BIOMEDICA/biomedica_webdataset_24M\"\n",
    "# subset = \"noncommercial\"\n",
    "\n",
    "\n",
    "# # Modified to process files from 000051 to 000100\n",
    "# start_file = 0\n",
    "# end_file = 5\n",
    "\n",
    "\n",
    "# # Create a custom dataset class that streams from HF\n",
    "# class HFStreamingDataset(IterableDataset):\n",
    "#     def __init__(self, url, token):\n",
    "#         self.url = url\n",
    "#         self.token = token\n",
    "\n",
    "\n",
    "#     def __iter__(self):\n",
    "#         # Stream the file with authentication\n",
    "#         headers = {\"Authorization\": f\"Bearer {self.token}\"}\n",
    "#         response = requests.get(self.url, headers=headers, stream=True)\n",
    "#         response.raise_for_status()\n",
    "\n",
    "\n",
    "#         # Use BytesIO to create a file-like object\n",
    "#         file_obj = io.BytesIO()\n",
    "\n",
    "\n",
    "#         # Buffer size for streaming\n",
    "#         buffer_size = 1024 * 1024  # 1MB chunks\n",
    "#         buffer = b\"\"\n",
    "\n",
    "\n",
    "#         # Create a tar file reader\n",
    "#         for chunk in response.iter_content(chunk_size=buffer_size):\n",
    "#             if not chunk:\n",
    "#                 continue\n",
    "\n",
    "\n",
    "#             buffer += chunk\n",
    "#             file_obj = io.BytesIO(buffer)\n",
    "\n",
    "\n",
    "#             try:\n",
    "#                 # Try to open as tar file\n",
    "#                 with tarfile.open(fileobj=file_obj, mode=\"r|*\") as tar:\n",
    "#                     for member in tar:\n",
    "#                         if member.isfile():\n",
    "#                             f = tar.extractfile(member)\n",
    "#                             if f:\n",
    "#                                 yield {\n",
    "#                                     \"filename\": member.name,\n",
    "#                                     \"data\": f.read()\n",
    "#                                 }\n",
    "#             except Exception as e:\n",
    "#                 # If we can't open it yet, continue buffering\n",
    "#                 print(f\"Buffering more data: {e}\")\n",
    "#                 continue\n",
    "\n",
    "\n",
    "# # Process all files in the range\n",
    "# for file_num in range(start_file, end_file + 1):\n",
    "#     pattern = f\"{file_num:06d}.tar\"  # Format with leading zeros: 000051.tar, etc.\n",
    "#     file_path = f\"{subset}/{pattern}\"\n",
    "   \n",
    "#     # Get the URL for the file\n",
    "#     url = hf_hub_url(repo_id=repo_id, filename=file_path, repo_type=\"dataset\")\n",
    "#     print(f\"Processing {pattern}...\")\n",
    "   \n",
    "#     try:\n",
    "#         # Create and use the dataset for this file\n",
    "#         dataset = HFStreamingDataset(url, hf_token)\n",
    "\n",
    "\n",
    "#         # Process items from this tar file\n",
    "#         image_count = 0\n",
    "#         for idx, item in enumerate(dataset):\n",
    "#             # Filter only .jpg or .jpeg files\n",
    "#             if item['filename'].lower().endswith(('.jpg', '.jpeg')):\n",
    "#                 print(f\"File {pattern}, Item {idx}: {item['filename']}\")\n",
    "               \n",
    "#                 # Convert the image data to a displayable format\n",
    "#                 image_data = io.BytesIO(item[\"data\"])\n",
    "#                 image = Image.open(image_data)\n",
    "\n",
    "\n",
    "#                 # Display the image\n",
    "#                 display.display(image)\n",
    "               \n",
    "#                 image_count += 1\n",
    "#                 if image_count >= 5:  # Show at most 5 images per tar file\n",
    "#                     break\n",
    "                   \n",
    "#     except Exception as e:\n",
    "#         print(f\"Error processing {pattern}: {e}\")\n",
    "#         continue\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1873b1f8-4cd6-4ffb-bfa8-2a636a21dfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import tkinter as tk\n",
    "# from tkinter import Label, Button\n",
    "# from PIL import Image, ImageTk\n",
    "\n",
    "# # List of folders to browse\n",
    "# folders = [\n",
    "#     #r\"D:\\Arogo 2\\useful biomedica data\\000002\",\n",
    "#     r\"D:\\Arogo 2\\useful biomedica data\\000003\",\n",
    "#     r\"D:\\Arogo 2\\useful biomedica data\\000004\",\n",
    "#     r\"D:\\Arogo 2\\useful biomedica data\\000051\"\n",
    "# ]\n",
    "\n",
    "# # Get all image files from the folders\n",
    "# image_extensions = {\".jpg\", \".jpeg\", \".png\"}\n",
    "# files = []\n",
    "# for folder in folders:\n",
    "#     if os.path.exists(folder):\n",
    "#         files.extend([os.path.join(folder, f) for f in os.listdir(folder) \n",
    "#                      if os.path.isfile(os.path.join(folder, f)) \n",
    "#                      and os.path.splitext(f)[1].lower() in image_extensions])\n",
    "\n",
    "# # Initialize file index\n",
    "# file_index = 0\n",
    "\n",
    "# def show_next_file():\n",
    "#     global file_index, img_label, info_label\n",
    "#     if files:\n",
    "#         current_file = files[file_index]\n",
    "#         # Get file name and folder path\n",
    "#         file_name = os.path.basename(current_file)\n",
    "#         folder_path = os.path.dirname(current_file)\n",
    "        \n",
    "#         # Update info label\n",
    "#         info_text = f\"File: {file_name}\\nFolder: {folder_path}\"\n",
    "#         info_label.config(text=info_text)\n",
    "        \n",
    "#         # Display image\n",
    "#         img = Image.open(current_file)\n",
    "#         img = img.resize((400, 400), Image.Resampling.LANCZOS)\n",
    "#         img = ImageTk.PhotoImage(img)\n",
    "#         img_label.config(image=img)\n",
    "#         img_label.image = img  # Keep reference to prevent garbage collection\n",
    "        \n",
    "#         file_index = (file_index + 1) % len(files)  # Loop back to start after last file\n",
    "#     else:\n",
    "#         img_label.config(text=\"No image files found.\")\n",
    "#         info_label.config(text=\"\")\n",
    "\n",
    "# # Create GUI window\n",
    "# root = tk.Tk()\n",
    "# root.title(\"Image Browser\")\n",
    "\n",
    "# # Create label for file information\n",
    "# info_label = Label(root, text=\"Press Next to browse images\", wraplength=500)\n",
    "# info_label.pack(pady=10)\n",
    "\n",
    "# # Create label to display images\n",
    "# img_label = Label(root)\n",
    "# img_label.pack(pady=10)\n",
    "\n",
    "# # Create Next button\n",
    "# next_button = Button(root, text=\"Next\", command=show_next_file)\n",
    "# next_button.pack(pady=10)\n",
    "\n",
    "# # Start GUI loop\n",
    "# root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3c9c1434-2efe-452c-a529-0908b223a0d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import json\n",
    "# from PIL import Image\n",
    "# from IPython.display import display, Image as IPImage\n",
    "\n",
    "# class MedicalImageFinder:\n",
    "#     def __init__(self, folders):\n",
    "#         self.folders = folders\n",
    "#         self.medical_keywords = [\n",
    "#             \"scan\", \"mri\", \" ct \", \"xray\", \"x-ray\", \"ultrasound\", \"radiograph\", \n",
    "#             \"imaging\", \"fmri\", \"pet scan\",\"dermatology\",\n",
    "#             \"skin\", \"lesion\", \"angiography\", \"endoscopy\",\n",
    "#             \"colonoscopy\", \"mammogram\", \"radiology\"\n",
    "#         ]\n",
    "\n",
    "#     def check_metadata_for_keywords(self, metadata):\n",
    "#         matched_keywords = set()\n",
    "#         fields_to_check = [\n",
    "#             'image_panel_type',\n",
    "#             'image_panel_subtype',\n",
    "#             'image_primary_label',\n",
    "#             'image_secondary_label',\n",
    "#         ]\n",
    "        \n",
    "#         for field in fields_to_check:\n",
    "#             if field in metadata:\n",
    "#                 field_value = metadata[field]\n",
    "#                 if isinstance(field_value, list):\n",
    "#                     field_text = ' '.join(str(item).lower() for item in field_value)\n",
    "#                 else:\n",
    "#                     field_text = str(field_value).lower()\n",
    "                \n",
    "#                 for keyword in self.medical_keywords:\n",
    "#                     if keyword in field_text:\n",
    "#                         matched_keywords.add(keyword)\n",
    "        \n",
    "#         return list(matched_keywords)\n",
    "\n",
    "#     def display_image_and_metadata(self):\n",
    "#         for folder in self.folders:\n",
    "#             if os.path.exists(folder):\n",
    "#                 files = os.listdir(folder)\n",
    "#                 count=0\n",
    "                \n",
    "#                 for file in files:\n",
    "#                     if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "#                         base_name = os.path.splitext(file)[0]\n",
    "#                         json_file = base_name + '.json'\n",
    "                        \n",
    "#                         if json_file in files:\n",
    "#                             json_path = os.path.join(folder, json_file)\n",
    "#                             image_path = os.path.join(folder, file)\n",
    "                            \n",
    "#                             try:\n",
    "#                                 # Read metadata\n",
    "#                                 with open(json_path, 'r', encoding='utf-8') as f:\n",
    "#                                     metadata = json.load(f)\n",
    "                                \n",
    "#                                 # Check for medical keywords\n",
    "#                                 matched_keywords = self.check_metadata_for_keywords(metadata)\n",
    "                                \n",
    "#                                 if matched_keywords:\n",
    "#                                     print(\"\\n\" + \"=\"*50)\n",
    "#                                     print(f\"Image File: {file}\")\n",
    "#                                     print(f\"Matched keywords: {', '.join(matched_keywords)}\")\n",
    "#                                     print(\"\\nMetadata:\")\n",
    "#                                     print(f\"Panel type: {metadata.get('image_panel_type', 'N/A')}\")\n",
    "#                                     print(f\"Panel subtype: {metadata.get('image_panel_subtype', 'N/A')}\")\n",
    "#                                     print(f\"Primary labels: {metadata.get('image_primary_label', 'N/A')}\")\n",
    "#                                     print(f\"Secondary labels: {metadata.get('image_secondary_label', 'N/A')}\")\n",
    "#                                     print(f\"Label ID: {metadata.get('image_label_id', 'N/A')}\")\n",
    "#                                     count=count+1\n",
    "#                                     print(\"=\"*50)\n",
    "                                    \n",
    "#                                     # Display image in Jupyter notebook\n",
    "#                                     try:\n",
    "#                                         display(IPImage(filename=image_path))\n",
    "#                                         print(\"\\n\")  # Add space after image\n",
    "#                                     except Exception as e:\n",
    "#                                         print(f\"Error displaying image: {str(e)}\")\n",
    "                            \n",
    "#                             except json.JSONDecodeError:\n",
    "#                                 print(f\"Error reading JSON for {file}\")\n",
    "#                             except Exception as e:\n",
    "#                                 print(f\"Error processing {file}: {str(e)}\")\n",
    "#                 print(folder,count)\n",
    "\n",
    "# # List of folders to browse\n",
    "# folders = [\n",
    "#     r\"D:\\Arogo 2\\useful biomedica data\\000002\",\n",
    "# ]\n",
    "\n",
    "# # Create finder and display images with metadata\n",
    "# finder = MedicalImageFinder(folders)\n",
    "# finder.display_image_and_metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e638f9ba-7444-4b97-a68c-0ead49cdc5ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON is valid\n"
     ]
    }
   ],
   "source": [
    "# import json\n",
    "\n",
    "# file_path = r\"D:\\Arogo 2\\useful biomedica data\\000002\\filelist_noncommercial_batch_0_0-PMC466984-0-MGR-14-133-g001.json\"\n",
    "\n",
    "# try:\n",
    "#     with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "#         data = json.load(f)\n",
    "#     print(\"JSON is valid\")\n",
    "# except json.JSONDecodeError as e:\n",
    "#     print(f\"JSON Decode Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "70bc6c4b-2c5c-4580-8e71-0082b9eeb2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import tarfile\n",
    "import json\n",
    "import os\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "from IPython.display import display, Image as IPImage\n",
    "\n",
    "class TarMedicalDownloader:\n",
    "    def __init__(self, hf_token, base_url=\"https://huggingface.co/datasets/BIOMEDICA/biomedica_webdataset_24M/resolve/main/noncommercial/\"):\n",
    "        self.base_url = base_url\n",
    "        self.medical_keywords = [\n",
    "            \"scan\", \"mri\", \"xray\", \"ultrasound\", \"radiograph\", \n",
    "            \"imaging\", \"fmri\", \"pet scan\", \"dermatology\",\n",
    "            \"skin\", \"lesion\", \"angiography\", \"endoscopy\",\n",
    "            \"colonoscopy\", \"mammogram\", \"radiology\",'computerized tomography'\n",
    "        ]\n",
    "        # Setup session with authentication\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({'Authorization': f'Bearer {hf_token}'})\n",
    "\n",
    "    def check_metadata_for_keywords(self, metadata):\n",
    "        matched_keywords = set()\n",
    "        fields_to_check = [\n",
    "            'image_panel_type',\n",
    "            'image_panel_subtype',\n",
    "            'image_primary_label',\n",
    "            'image_secondary_label',\n",
    "        ]\n",
    "        \n",
    "        for field in fields_to_check:\n",
    "            if field in metadata:\n",
    "                field_value = metadata[field]\n",
    "                if isinstance(field_value, list):\n",
    "                    field_text = ' '.join(str(item).lower() for item in field_value)\n",
    "                else:\n",
    "                    field_text = str(field_value).lower()\n",
    "                \n",
    "                for keyword in self.medical_keywords:\n",
    "                    if keyword in field_text:\n",
    "                        matched_keywords.add(keyword)\n",
    "        \n",
    "        return list(matched_keywords)\n",
    "\n",
    "    def save_file(self, file_data, output_path):\n",
    "        \"\"\"Save file data to disk\"\"\"\n",
    "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "        with open(output_path, 'wb') as f:\n",
    "            f.write(file_data)\n",
    "\n",
    "    def process_and_download_tar(self, tar_number, output_dir=\"downloaded_medical_data\", file_format=\"{:06d}\"):\n",
    "        \"\"\"\n",
    "        Process a TAR file and download matching files\n",
    "        \"\"\"\n",
    "        formatted_num = file_format.format(tar_number)\n",
    "        tar_url = f\"{self.base_url}{formatted_num}.tar\"\n",
    "        \n",
    "        print(f\"Fetching TAR file: {tar_url}\")\n",
    "        \n",
    "        try:\n",
    "            # Download the TAR file with authentication\n",
    "            response = self.session.get(tar_url, stream=True)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # Process the TAR file in memory\n",
    "            tar_bytes = BytesIO(response.content)\n",
    "            count = 0\n",
    "            \n",
    "            with tarfile.open(fileobj=tar_bytes, mode='r') as tar:\n",
    "                # Get all file names in the TAR\n",
    "                members = tar.getmembers()\n",
    "                json_files = [m for m in members if m.name.endswith('.json')]\n",
    "                \n",
    "                for json_member in json_files:\n",
    "                    try:\n",
    "                        # Extract JSON data and store it\n",
    "                        json_file = tar.extractfile(json_member)\n",
    "                        if not json_file:\n",
    "                            continue\n",
    "                            \n",
    "                        # Read the JSON content and store it\n",
    "                        json_content = json_file.read()\n",
    "                        metadata = json.loads(json_content)\n",
    "                        \n",
    "                        # Check for medical keywords\n",
    "                        matched_keywords = self.check_metadata_for_keywords(metadata)\n",
    "                        if not matched_keywords:\n",
    "                            continue\n",
    "                            \n",
    "                        # Get base name for related files\n",
    "                        base_name = json_member.name.rsplit('.', 1)[0]\n",
    "                        \n",
    "                        # Find corresponding image and text files\n",
    "                        image_member = None\n",
    "                        for ext in ['.jpg', '.png']:\n",
    "                            try:\n",
    "                                image_member = tar.getmember(base_name + ext)\n",
    "                                image_ext = ext\n",
    "                                break\n",
    "                            except KeyError:\n",
    "                                continue\n",
    "                                \n",
    "                        try:\n",
    "                            text_member = tar.getmember(base_name + '.txt')\n",
    "                        except KeyError:\n",
    "                            text_member = None\n",
    "                            \n",
    "                        if not image_member:\n",
    "                            continue\n",
    "                            \n",
    "                        # Create directory for this match\n",
    "                        match_dir = os.path.join(output_dir, f\"match_{formatted_num}_{base_name.replace('/', '_')}\")\n",
    "                        \n",
    "                        # Save JSON using the stored content\n",
    "                        json_path = os.path.join(match_dir, f\"metadata.json\")\n",
    "                        self.save_file(json_content, json_path)\n",
    "                        \n",
    "                        # Save image\n",
    "                        image_file = tar.extractfile(image_member)\n",
    "                        if image_file:\n",
    "                            image_path = os.path.join(match_dir, f\"image{image_ext}\")\n",
    "                            self.save_file(image_file.read(), image_path)\n",
    "                        \n",
    "                        # Save text if exists\n",
    "                        if text_member:\n",
    "                            text_file = tar.extractfile(text_member)\n",
    "                            if text_file:\n",
    "                                text_path = os.path.join(match_dir, \"text.txt\")\n",
    "                                self.save_file(text_file.read(), text_path)\n",
    "                        \n",
    "                        count += 1\n",
    "                            \n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing file {json_member.name}: {str(e)}\")\n",
    "                        continue\n",
    "                        \n",
    "            print(f\"Total matches found and downloaded from TAR {formatted_num}: {count}\")\n",
    "            \n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error downloading TAR file: {str(e)}\")\n",
    "        except tarfile.TarError as e:\n",
    "            print(f\"Error processing TAR file: {str(e)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error: {str(e)}\")\n",
    "\n",
    "    def process_tar_range(self, start_num, end_num, output_dir=\"downloaded_medical_data\", file_format=\"{:06d}\"):\n",
    "        \"\"\"\n",
    "        Process a range of TAR files and download matching files\n",
    "        \"\"\"\n",
    "        for num in range(start_num, end_num + 1):\n",
    "            self.process_and_download_tar(num, output_dir, file_format)\n",
    "\n",
    "    def add_custom_keywords(self, keywords):\n",
    "        \"\"\"Add additional keywords to search for\"\"\"\n",
    "        if isinstance(keywords, list):\n",
    "            self.medical_keywords.extend(keywords)\n",
    "        else:\n",
    "            self.medical_keywords.append(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2818fcdb-04f1-404f-bf11-902deb84b7a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching TAR file: https://huggingface.co/datasets/BIOMEDICA/biomedica_webdataset_24M/resolve/main/noncommercial/000000.tar\n",
      "Total matches found and downloaded from TAR 000000: 0\n",
      "Fetching TAR file: https://huggingface.co/datasets/BIOMEDICA/biomedica_webdataset_24M/resolve/main/noncommercial/000001.tar\n",
      "Total matches found and downloaded from TAR 000001: 608\n",
      "Fetching TAR file: https://huggingface.co/datasets/BIOMEDICA/biomedica_webdataset_24M/resolve/main/noncommercial/000002.tar\n",
      "Total matches found and downloaded from TAR 000002: 10000\n",
      "Fetching TAR file: https://huggingface.co/datasets/BIOMEDICA/biomedica_webdataset_24M/resolve/main/noncommercial/000003.tar\n",
      "Total matches found and downloaded from TAR 000003: 3430\n",
      "Fetching TAR file: https://huggingface.co/datasets/BIOMEDICA/biomedica_webdataset_24M/resolve/main/noncommercial/000004.tar\n",
      "Total matches found and downloaded from TAR 000004: 10000\n",
      "Fetching TAR file: https://huggingface.co/datasets/BIOMEDICA/biomedica_webdataset_24M/resolve/main/noncommercial/000005.tar\n",
      "Total matches found and downloaded from TAR 000005: 3500\n",
      "Fetching TAR file: https://huggingface.co/datasets/BIOMEDICA/biomedica_webdataset_24M/resolve/main/noncommercial/000006.tar\n",
      "Total matches found and downloaded from TAR 000006: 0\n",
      "Fetching TAR file: https://huggingface.co/datasets/BIOMEDICA/biomedica_webdataset_24M/resolve/main/noncommercial/000007.tar\n",
      "Total matches found and downloaded from TAR 000007: 0\n",
      "Fetching TAR file: https://huggingface.co/datasets/BIOMEDICA/biomedica_webdataset_24M/resolve/main/noncommercial/000008.tar\n",
      "Total matches found and downloaded from TAR 000008: 0\n",
      "Fetching TAR file: https://huggingface.co/datasets/BIOMEDICA/biomedica_webdataset_24M/resolve/main/noncommercial/000009.tar\n",
      "Total matches found and downloaded from TAR 000009: 6762\n",
      "Fetching TAR file: https://huggingface.co/datasets/BIOMEDICA/biomedica_webdataset_24M/resolve/main/noncommercial/000010.tar\n",
      "Total matches found and downloaded from TAR 000010: 914\n"
     ]
    }
   ],
   "source": [
    "# # Initialize with your HuggingFace token\n",
    "hf_token = \"PUT KEY HERE\"#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\n",
    "downloader = TarMedicalDownloader(hf_token)\n",
    "\n",
    "# # Download from a single TAR file\n",
    "#downloader.process_and_download_tar(1)\n",
    "\n",
    "# # Or download from multiple TAR files\n",
    "downloader.process_tar_range(0, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "460bbd9e-d012-4f93-8df6-165262de26c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ No issues found in JSON files\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def validate_json_files(directory_path):\n",
    "    \"\"\"\n",
    "    Validate JSON files in the specified directory and its subdirectories.\n",
    "    Returns a list of problematic files and their issues.\n",
    "    \"\"\"\n",
    "    issues = []\n",
    "    \n",
    "    for root, dirs, files in os.walk(directory_path):\n",
    "        for file in files:\n",
    "            if file.endswith('.json'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                try:\n",
    "                    # Check if file is empty\n",
    "                    if os.path.getsize(file_path) == 0:\n",
    "                        issues.append({\n",
    "                            'file': file_path,\n",
    "                            'issue': 'Empty file',\n",
    "                            'size': 0\n",
    "                        })\n",
    "                        continue\n",
    "                        \n",
    "                    # Try to read and parse JSON\n",
    "                    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                        content = f.read()\n",
    "                        if not content.strip():\n",
    "                            issues.append({\n",
    "                                'file': file_path,\n",
    "                                'issue': 'File contains only whitespace',\n",
    "                                'size': len(content)\n",
    "                            })\n",
    "                            continue\n",
    "                            \n",
    "                        # Try to parse JSON\n",
    "                        data = json.loads(content)\n",
    "                        \n",
    "                        # Check if JSON is empty object or array\n",
    "                        if not data and isinstance(data, (dict, list)):\n",
    "                            issues.append({\n",
    "                                'file': file_path,\n",
    "                                'issue': 'Empty JSON object/array',\n",
    "                                'size': len(content)\n",
    "                            })\n",
    "                            continue\n",
    "                            \n",
    "                        # Check for minimum expected fields\n",
    "                        if isinstance(data, dict):\n",
    "                            required_fields = [\n",
    "                                'image_panel_type',\n",
    "                                'image_panel_subtype',\n",
    "                                'image_primary_label',\n",
    "                                'image_secondary_label'\n",
    "                            ]\n",
    "                            missing_fields = [field for field in required_fields if field not in data]\n",
    "                            if missing_fields:\n",
    "                                issues.append({\n",
    "                                    'file': file_path,\n",
    "                                    'issue': f'Missing required fields: {\", \".join(missing_fields)}',\n",
    "                                    'size': len(content)\n",
    "                                })\n",
    "                                \n",
    "                except json.JSONDecodeError as e:\n",
    "                    issues.append({\n",
    "                        'file': file_path,\n",
    "                        'issue': f'Invalid JSON format: {str(e)}',\n",
    "                        'size': os.path.getsize(file_path)\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    issues.append({\n",
    "                        'file': file_path,\n",
    "                        'issue': f'Error processing file: {str(e)}',\n",
    "                        'size': os.path.getsize(file_path)\n",
    "                    })\n",
    "    \n",
    "    return issues\n",
    "\n",
    "def print_validation_report(issues):\n",
    "    \"\"\"\n",
    "    Print a formatted report of JSON validation issues\n",
    "    \"\"\"\n",
    "    if not issues:\n",
    "        print(\"✅ No issues found in JSON files\")\n",
    "        return\n",
    "        \n",
    "    print(f\"⚠️ Found {len(issues)} issues in JSON files:\\n\")\n",
    "    \n",
    "    for i, issue in enumerate(issues, 1):\n",
    "        print(f\"{i}. File: {issue['file']}\")\n",
    "        print(f\"   Issue: {issue['issue']}\")\n",
    "        print(f\"   Size: {issue['size']} bytes\")\n",
    "        print()\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Example directory path - replace with your actual path\n",
    "    directory = \"downloaded_medical_data\"\n",
    "    \n",
    "    if os.path.exists(directory):\n",
    "        issues = validate_json_files(directory)\n",
    "        print_validation_report(issues)\n",
    "    else:\n",
    "        print(f\"Directory '{directory}' not found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2d9a80-f34f-4bef-9bea-31a83f9c06f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
